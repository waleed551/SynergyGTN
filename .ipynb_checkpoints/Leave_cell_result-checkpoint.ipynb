{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [09:45:03] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "[09:45:03] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric import data as DATA\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json,pickle\n",
    "from collections import OrderedDict\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import (InMemoryDataset, Data, download_url,\n",
    "                                  extract_gz)\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "except ImportError:\n",
    "    Chem = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_features(mol):\n",
    "    symbols = ['K', 'Y', 'V', 'Sm', 'Dy', 'In', 'Lu', 'Hg', 'Co', 'Mg',    #list of all elements in the dataset\n",
    "        'Cu', 'Rh', 'Hf', 'O', 'As', 'Ge', 'Au', 'Mo', 'Br', 'Ce', \n",
    "        'Zr', 'Ag', 'Ba', 'N', 'Cr', 'Sr', 'Fe', 'Gd', 'I', 'Al', \n",
    "        'B', 'Se', 'Pr', 'Te', 'Cd', 'Pd', 'Si', 'Zn', 'Pb', 'Sn', \n",
    "        'Cl', 'Mn', 'Cs', 'Na', 'S', 'Ti', 'Ni', 'Ru', 'Ca', 'Nd', \n",
    "        'W', 'H', 'Li', 'Sb', 'Bi', 'La', 'Pt', 'Nb', 'P', 'F', 'C',\n",
    "        'Re','Ta','Ir','Be','Tl']\n",
    "\n",
    "    hybridizations = [\n",
    "        Chem.rdchem.HybridizationType.S,\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2,\n",
    "        'other',\n",
    "    ]\n",
    "\n",
    "    stereos = [\n",
    "        Chem.rdchem.BondStereo.STEREONONE,\n",
    "        Chem.rdchem.BondStereo.STEREOANY,\n",
    "        Chem.rdchem.BondStereo.STEREOZ,\n",
    "        Chem.rdchem.BondStereo.STEREOE,\n",
    "    ]\n",
    "    features = []\n",
    "    xs = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        symbol = [0.] * len(symbols)\n",
    "        symbol[symbols.index(atom.GetSymbol())] = 1.\n",
    "        #comment degree from 6 to 8\n",
    "        degree = [0.] * 8\n",
    "        degree[atom.GetDegree()] = 1.\n",
    "        formal_charge = atom.GetFormalCharge()\n",
    "        radical_electrons = atom.GetNumRadicalElectrons()\n",
    "        hybridization = [0.] * len(hybridizations)\n",
    "        hybridization[hybridizations.index(\n",
    "            atom.GetHybridization())] = 1.\n",
    "        aromaticity = 1. if atom.GetIsAromatic() else 0.\n",
    "        hydrogens = [0.] * 5\n",
    "        hydrogens[atom.GetTotalNumHs()] = 1.\n",
    "        chirality = 1. if atom.HasProp('_ChiralityPossible') else 0.\n",
    "        chirality_type = [0.] * 2\n",
    "        if atom.HasProp('_CIPCode'):\n",
    "            chirality_type[['R', 'S'].index(atom.GetProp('_CIPCode'))] = 1.\n",
    "    \n",
    "        x = torch.tensor(symbol + degree + [formal_charge] +\n",
    "                         [radical_electrons] + hybridization +\n",
    "                         [aromaticity] + hydrogens + [chirality] +\n",
    "                         chirality_type)\n",
    "        xs.append(x)\n",
    "    \n",
    "        x = torch.stack(xs, dim=0)\n",
    "\n",
    "    edge_indices = []\n",
    "    edge_attrs = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
    "        edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
    "    \n",
    "        bond_type = bond.GetBondType()\n",
    "        single = 1. if bond_type == Chem.rdchem.BondType.SINGLE else 0.\n",
    "        double = 1. if bond_type == Chem.rdchem.BondType.DOUBLE else 0.\n",
    "        triple = 1. if bond_type == Chem.rdchem.BondType.TRIPLE else 0.\n",
    "        aromatic = 1. if bond_type == Chem.rdchem.BondType.AROMATIC else 0.\n",
    "        conjugation = 1. if bond.GetIsConjugated() else 0.\n",
    "        ring = 1. if bond.IsInRing() else 0.\n",
    "        stereo = [0.] * 4\n",
    "        stereo[stereos.index(bond.GetStereo())] = 1.\n",
    "    \n",
    "        edge_attr = torch.tensor(\n",
    "            [single, double, triple, aromatic, conjugation, ring] + stereo)\n",
    "    \n",
    "        edge_attrs += [edge_attr, edge_attr]\n",
    "    \n",
    "    if len(edge_attrs) == 0:\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_indices).t().contiguous()\n",
    "        edge_attr = torch.stack(edge_attrs, dim=0)\n",
    "    return x, edge_index, edge_attr\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    if mol == None:\n",
    "        return None\n",
    "    \n",
    "    c_size = mol.GetNumAtoms()\n",
    "    features, edge_index, edge_attr = smiles_features(mol)\n",
    "    # features = []\n",
    "    # bonds = mol.GetBonds()\n",
    "    # for atom in mol.GetAtoms():\n",
    "    #     feature = atom_features(atom)\n",
    "    #     features.append( feature / sum(feature) )\n",
    "\n",
    "    # edges = []\n",
    "    # for bond in mol.GetBonds():\n",
    "    #     edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    # g = nx.Graph(edges).to_directed()\n",
    "    # edge_index = []\n",
    "    # for e1, e2 in g.edges:\n",
    "    #     edge_index.append([e1, e2])\n",
    "        \n",
    "    return features, edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Molecule_data(InMemoryDataset):\n",
    "    def __init__(self, root='/tmp', dataset='_drug1',xd=None, xt=None, y=None, xt_featrue=None, transform=None,\n",
    "                 pre_transform=None,smile_graph=None):\n",
    "\n",
    "        #root is required for save preprocessed data, default is '/tmp'\n",
    "        super(Molecule_data, self).__init__(root, transform, pre_transform)\n",
    "        # benchmark dataset, default = 'davis'\n",
    "        self.dataset = dataset\n",
    "        if os.path.isfile(self.processed_paths[0]):\n",
    "#             print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        else:\n",
    "            print('Pre-processed data {} not found, doing pre-processing...'.format(self.processed_paths[0]))\n",
    "            self.process(xd, xt, xt_featrue, y, smile_graph)\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        pass\n",
    "        #return ['some_file_1', 'some_file_2', ...]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.dataset + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def _download(self):\n",
    "        pass\n",
    "\n",
    "    def _process(self):\n",
    "        if not os.path.exists(self.processed_dir):\n",
    "            os.makedirs(self.processed_dir)\n",
    "            \n",
    "    def get_cell_feature(self, cellId, cell_features):\n",
    "        for row in islice(cell_features, 0, None):\n",
    "            if cellId in row[0]:\n",
    "                return row[1:]\n",
    "        return False\n",
    "\n",
    "    def process(self, xd, xt, xt_featrue,y,smile_graph):\n",
    "        assert (len(xd) == len(xt) and len(xt) == len(y)), \"The three lists must be the same length!\"\n",
    "        data_list = []\n",
    "        data_len = len(xd)\n",
    "        for i in range(data_len):\n",
    "            print('Converting SMILES to graph: {}/{}'.format(i+1, data_len))\n",
    "            smiles = xd[i]\n",
    "            target = xt[i]\n",
    "            labels = y[i]\n",
    "            # convert SMILES to molecular representation using rdkit\n",
    "            x, edge_index, edge_attr = smile_graph[smiles]\n",
    "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
    "            GCNData = Data(x=torch.Tensor(x),\n",
    "                      edge_index=edge_index,\n",
    "                      edge_attr=edge_attr,\n",
    "                      y=torch.FloatTensor([labels]))\n",
    "        \n",
    "            cell = self.get_cell_feature(target, xt_featrue)\n",
    "\n",
    "            if cell == False : \n",
    "                print('cell', cell)\n",
    "                sys.exit()\n",
    "\n",
    "            new_cell = []\n",
    "            # print('cell_feature', cell_feature)\n",
    "            for n in cell:\n",
    "                new_cell.append(float(n))\n",
    "            GCNData.cell = torch.FloatTensor([new_cell])\n",
    "            #GCNData.__setitem__('c_size', torch.LongTensor([c_size]))\n",
    "            # append graph, label and target sequence to data list\n",
    "            data_list.append(GCNData)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "        print('Graph construction done. Saving to file.')\n",
    "        data, slices = self.collate(data_list)\n",
    "#         print(data.shape,slices.shape)\n",
    "        # save preprocessed data:\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_features [['gene_id' 'ENSG00000116237' 'ENSG00000162413' ... 'ENSG00000157617'\n",
      "  'ENSG00000160208' 'ENSG00000141959']\n",
      " ['transcript_id' 'ENST00000343813' 'ENST00000377658' ...\n",
      "  'ENST00000329623' 'ENST00000340648' 'ENST00000349048']\n",
      " ['A2058' '51.07' '20.76' ... '1.46' '23.67' '78.86']\n",
      " ...\n",
      " ['UACC62' '19.16' '19.6' ... '0' '17.3' '60.09']\n",
      " ['VCAP' '50.7' '7.41' ... '3.75' '28.45' '72.63']\n",
      " ['ZR751' '30.69' '4.72' ... '0.77' '6.8' '36.37']]\n",
      "开始创建数据\n",
      "创建数据成功\n",
      "preparing  new_labels_0_10_.pt in pytorch format!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import islice\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json, pickle\n",
    "from collections import OrderedDict\n",
    "from rdkit import Chem\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def creat_data(datafile, cellfile):\n",
    "    file2 = cellfile\n",
    "    cell_features = []\n",
    "    with open(file2) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)  # 使用csv.reader读取csvfile中的文件\n",
    "        for row in csv_reader:\n",
    "            cell_features.append(row)\n",
    "    cell_features = np.array(cell_features)\n",
    "    print('cell_features', cell_features)\n",
    "\n",
    "    compound_iso_smiles = []\n",
    "    df = pd.read_csv('data/smiles.csv')\n",
    "    compound_iso_smiles += list(df['smile'])\n",
    "    compound_iso_smiles = set(compound_iso_smiles)\n",
    "    smile_graph = {}\n",
    "    for smile in compound_iso_smiles:\n",
    "        g = smile_to_graph(smile)\n",
    "        smile_graph[smile] = g\n",
    "\n",
    "    datasets = datafile\n",
    "    # convert to PyTorch data format\n",
    "    processed_data_file_train = 'data_perparation/processed/' + datasets + '_train.pt'\n",
    "\n",
    "    if ((not os.path.isfile(processed_data_file_train))):\n",
    "        df = pd.read_csv('data/' + datasets + '.csv')\n",
    "        drug1, drug2, cell, label = list(df['drug1']), list(df['drug2']), list(df['cell']), list(df['label'])\n",
    "        drug1, drug2, cell, label = np.asarray(drug1), np.asarray(drug2), np.asarray(cell), np.asarray(label)\n",
    "        # make data PyTorch Geometric ready\n",
    "\n",
    "        print('开始创建数据')\n",
    "        Molecule_data(root='data_perparation', dataset=datafile + '_drug1', xd=drug1, xt=cell, xt_featrue=cell_features, y=label,smile_graph=smile_graph)\n",
    "        Molecule_data(root='data_perparation', dataset=datafile + '_drug2', xd=drug2, xt=cell, xt_featrue=cell_features, y=label,smile_graph=smile_graph)\n",
    "        print('创建数据成功')\n",
    "        print('preparing ', datasets + '_.pt in pytorch format!')\n",
    "    #\n",
    "    #     print(processed_data_file_train, ' have been created')\n",
    "    #\n",
    "    # else:\n",
    "    #     print(processed_data_file_train, ' are already created')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # datafile = 'prostate'\n",
    "    cellfile = 'data/new_cell_features_954.csv'\n",
    "    da = ['new_labels_0_10']\n",
    "    for datafile in da:\n",
    "        creat_data(datafile, cellfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import max_pool as mxp\n",
    "from torch_geometric.nn import avg_pool as agp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from heatmap import get_map\n",
    "\n",
    "\n",
    "\n",
    "# GAT  model\n",
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self, num_features_xd=92, n_output=2, num_features_xt=954, output_dim=128, dropout=0.05, file=None):\n",
    "        super(GATNet, self).__init__()\n",
    "\n",
    "        # graph drug layers\n",
    "        self.drug1_gcn1 = GATConv(num_features_xd, output_dim, heads=10,edge_dim=10, dropout=dropout)\n",
    "        self.drug1_gcn2 = GATConv(output_dim *10, output_dim,edge_dim=10, dropout=dropout)\n",
    "        # self.drug1_gcn3 = GATConv(output_dim, output_dim, dropout=dropout)\n",
    "        self.drug1_fc_g1 = nn.Linear(output_dim, output_dim)\n",
    "        # self.drug1_fc_g2 = nn.Linear(2048, output_dim)\n",
    "        self.filename = file\n",
    "\n",
    "\n",
    "        # DL cell featrues\n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Linear(num_features_xt, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, output_dim * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(output_dim * 4, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        #self.fc3 = nn.Linear(256, 128)\n",
    "        self.out = nn.Linear(32, n_output)\n",
    "\n",
    "        # activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def get_col_index(self, x):\n",
    "        row_size = len(x[:, 0])\n",
    "        row = np.zeros(row_size)\n",
    "        col_size = len(x[0, :])\n",
    "        for i in range(col_size):\n",
    "            row[np.argmax(x[:, i])] += 1\n",
    "        return row\n",
    "\n",
    "    def save_num(self, d, path):\n",
    "        d = d.cpu().numpy()\n",
    "        ind = self.get_col_index(d)\n",
    "        ind = pd.DataFrame(ind)\n",
    "        ind.to_csv('data/case_study/' + path + '_index.csv', header=0, index=0)\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        x1, edge_index1,edge_attr1, batch1, cell = data1.x.float(), data1.edge_index,data1.edge_attr, data1.batch, data1.cell\n",
    "        \n",
    "        x2, edge_index2,edge_attr2, batch2 = data2.x.float(), data2.edge_index,data2.edge_attr, data2.batch\n",
    "        # deal drug1\n",
    "        x1 = self.drug1_gcn1(x1, edge_index=edge_index1,edge_attr=edge_attr1)\n",
    "        x1 = F.relu(x1)\n",
    "        #x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "        x1 = self.drug1_gcn2(x1, edge_index=edge_index1,edge_attr=edge_attr1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "    \n",
    "        x1 = gmp(x1,batch1)         # global max pooling\n",
    "\n",
    "\n",
    "        x1 = self.drug1_fc_g1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        # x1 = self.drug1_fc_g2(x1)\n",
    "        # x1 = self.relu(x1)\n",
    "\n",
    "        # deal drug2\n",
    "    \n",
    "        x2 = self.drug1_gcn1(x2, edge_index2,edge_attr=edge_attr2)\n",
    "        x2 = F.relu(x2)\n",
    "        #x2 = F.dropout(x2, p=0.1, training=self.training)\n",
    "        x2 = self.drug1_gcn2(x2, edge_index2,edge_attr=edge_attr2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = F.dropout(x2, p=0.1, training=self.training)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        x2 = gmp(x2,batch2)  # global max pooling\n",
    "\n",
    "\n",
    "        x2 = self.drug1_fc_g1(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        # x2 = self.drug1_fc_g2(x2)\n",
    "        # x2 = self.relu(x2)\n",
    "\n",
    "        # deal cell\n",
    "        cell = F.normalize(cell, 2, 1)\n",
    "        cell_vector = self.reduction(cell)\n",
    "\n",
    "\n",
    "\n",
    "        # concat\n",
    "        xc = torch.cat((x1, x2, cell_vector), 1)\n",
    "        xc = F.normalize(xc, 2, 1)\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        #xc = self.dropout(xc)\n",
    "        #xc = self.fc3(xc)\n",
    "        #xc = self.relu(xc)\n",
    "        #xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import GATConv,TransformerConv\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import max_pool as mxp\n",
    "from torch_geometric.nn import avg_pool as agp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from heatmap import get_map\n",
    "\n",
    "\n",
    "\n",
    "# GAT  model\n",
    "class TransfGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features_xd=92, n_output=2, num_features_xt=954, output_dim=128, dropout=0.05, file=None):\n",
    "        super(TransfGNN, self).__init__()\n",
    "\n",
    "        # graph drug layers\n",
    "        self.drug1_gcn1 = TransformerConv(num_features_xd, output_dim, heads=10,edge_dim=10, dropout=dropout)\n",
    "        self.drug1_gcn2 = TransformerConv(output_dim *10, output_dim,edge_dim=10, dropout=dropout)\n",
    "        # self.drug1_gcn3 = GATConv(output_dim, output_dim, dropout=dropout)\n",
    "        self.drug1_fc_g1 = nn.Linear(output_dim, output_dim)\n",
    "        # self.drug1_fc_g2 = nn.Linear(2048, output_dim)\n",
    "        self.filename = file\n",
    "\n",
    "\n",
    "        # DL cell featrues\n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Linear(num_features_xt, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, output_dim * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(output_dim * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        #self.fc3 = nn.Linear(256, 128)\n",
    "        self.out = nn.Linear(128, n_output)\n",
    "\n",
    "        # activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def get_col_index(self, x):\n",
    "        row_size = len(x[:, 0])\n",
    "        row = np.zeros(row_size)\n",
    "        col_size = len(x[0, :])\n",
    "        for i in range(col_size):\n",
    "            row[np.argmax(x[:, i])] += 1\n",
    "        return row\n",
    "\n",
    "    def save_num(self, d, path):\n",
    "        d = d.cpu().numpy()\n",
    "        ind = self.get_col_index(d)\n",
    "        ind = pd.DataFrame(ind)\n",
    "        ind.to_csv('data/case_study/' + path + '_index.csv', header=0, index=0)\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        x1, edge_index1,edge_attr1, batch1, cell = data1.x.float(), data1.edge_index,data1.edge_attr, data1.batch, data1.cell\n",
    "        \n",
    "        x2, edge_index2,edge_attr2, batch2 = data2.x.float(), data2.edge_index,data2.edge_attr, data2.batch\n",
    "        # deal drug1\n",
    "        x1 = self.drug1_gcn1(x1, edge_index=edge_index1,edge_attr=edge_attr1)\n",
    "        x1 = F.relu(x1)\n",
    "        #x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "        x1 = self.drug1_gcn2(x1, edge_index=edge_index1,edge_attr=edge_attr1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "    \n",
    "        x1 = gmp(x1,batch1)         # global max pooling\n",
    "\n",
    "\n",
    "        x1 = self.drug1_fc_g1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        # x1 = self.drug1_fc_g2(x1)\n",
    "        # x1 = self.relu(x1)\n",
    "\n",
    "        # deal drug2\n",
    "    \n",
    "        x2 = self.drug1_gcn1(x2, edge_index2,edge_attr=edge_attr2)\n",
    "        x2 = F.relu(x2)\n",
    "        #x2 = F.dropout(x2, p=0.1, training=self.training)\n",
    "        x2 = self.drug1_gcn2(x2, edge_index2,edge_attr=edge_attr2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = F.dropout(x2, p=0.1, training=self.training)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        x2 = gmp(x2,batch2)  # global max pooling\n",
    "\n",
    "\n",
    "        x2 = self.drug1_fc_g1(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        # x2 = self.drug1_fc_g2(x2)\n",
    "        # x2 = self.relu(x2)\n",
    "\n",
    "        # deal cell\n",
    "        cell = F.normalize(cell, 2, 1)\n",
    "        cell_vector = self.reduction(cell)\n",
    "\n",
    "\n",
    "\n",
    "        # concat\n",
    "        xc = torch.cat((x1, x2, cell_vector), 1)\n",
    "        xc = F.normalize(xc, 2, 1)\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        #xc = self.dropout(xc)\n",
    "        #xc = self.fc3(xc)\n",
    "        #xc = self.relu(xc)\n",
    "        #xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "#from models.gat import GATNet\n",
    "from models.gat_gcn_test import GAT_GCN\n",
    "from models.attentionFP import AttentiveFP\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "from utils_test import *\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, roc_auc_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# training function at each epoch\n",
    "def train(model, device, drug1_loader_train, drug2_loader_train, optimizer, epoch):\n",
    "    print('Training on {} samples...'.format(len(drug1_loader_train.dataset)))\n",
    "    model.train()\n",
    "    # train_loader = np.array(train_loader)\n",
    "    for batch_idx, data in enumerate(zip(drug1_loader_train, drug2_loader_train)):\n",
    "        data1 = data[0]\n",
    "        data2 = data[1]\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        y = data[0].y.view(-1, 1).long().to(device)\n",
    "        y = y.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data1, data2)\n",
    "        loss = loss_fn(output, y)\n",
    "        # print('loss', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                                                           batch_idx * len(data1.x),\n",
    "                                                                           len(drug1_loader_train.dataset),\n",
    "                                                                           100. * batch_idx / len(drug1_loader_train),\n",
    "                                                                           loss.item()))\n",
    "\n",
    "\n",
    "def predicting(model, device, drug1_loader_test, drug2_loader_test):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    total_prelabels = torch.Tensor()\n",
    "    print('Make prediction for {} samples...'.format(len(drug1_loader_test.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for data in zip(drug1_loader_test, drug2_loader_test):\n",
    "            data1 = data[0]\n",
    "            data2 = data[1]\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            output = model(data1, data2)\n",
    "            ys = F.softmax(output, 1).to('cpu').data.numpy()\n",
    "            predicted_labels = list(map(lambda x: np.argmax(x), ys))\n",
    "            predicted_scores = list(map(lambda x: x[1], ys))\n",
    "            total_preds = torch.cat((total_preds, torch.Tensor(predicted_scores)), 0)\n",
    "            total_prelabels = torch.cat((total_prelabels, torch.Tensor(predicted_labels)), 0)\n",
    "            total_labels = torch.cat((total_labels, data1.y.view(-1, 1).cpu()), 0)\n",
    "    return total_labels.numpy().flatten(), total_preds.numpy().flatten(), total_prelabels.numpy().flatten()\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "    n = int(len(dataset) * ratio)\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2\n",
    "\n",
    "# modeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\n",
    "# datasets = [['davis', 'kiba'][int(sys.argv[1])]]\n",
    "# model_st = modeling.__name__\n",
    "\n",
    "\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 256\n",
    "LR = 0.0005\n",
    "LOG_INTERVAL = 20\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "print('Learning rate: ', LR)\n",
    "print('Epochs: ', NUM_EPOCHS)\n",
    "datafile = 'new_labels_0_10'\n",
    "\n",
    "# CPU or GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('The code uses GPU...')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('The code uses CPU!!!')\n",
    "\n",
    "drug1_data = TestbedDataset(root='data_perparation', dataset=datafile + '_drug1')\n",
    "drug2_data = TestbedDataset(root='data_perparation', dataset=datafile + '_drug2')\n",
    "\n",
    "lenth = len(drug1_data)\n",
    "pot = int(lenth/5)\n",
    "print('lenth', lenth)\n",
    "print('pot', pot)\n",
    "\n",
    "results =[]\n",
    "random_num = random.sample(range(0, lenth), lenth)\n",
    "for i in range(5):\n",
    "    test_num = random_num[pot*i:pot*(i+1)]\n",
    "    train_num = random_num[:pot*i] + random_num[pot*(i+1):]\n",
    "\n",
    "    drug1_data_train = drug1_data[train_num[pot*i:pot*(i+1)]]\n",
    "    drug1_data_test = drug1_data[test_num]\n",
    "    torch.save(drug1_data_train,'data_perparation/folds/Drug1/fold_'+str(i)+'1_'+'x_train.pt')\n",
    "    torch.save(drug1_data_test,'data_perparation/folds/Drug1/fold_'+str(i)+'1_'+'x_test.pt')\n",
    "    # print('type(drug1_data_train)', type(drug1_data_train))\n",
    "    # print('drug1_data_train[0]', drug1_data_train[0])\n",
    "    # print('len(drug1_data_train)', len(drug1_data_train))\n",
    "    drug1_loader_train = DataLoader(drug1_data_train, batch_size=TRAIN_BATCH_SIZE, shuffle=None)\n",
    "    drug1_loader_test = DataLoader(drug1_data_test, batch_size=TRAIN_BATCH_SIZE, shuffle=None)\n",
    "\n",
    "\n",
    "    drug2_data_test = drug2_data[test_num]\n",
    "    drug2_data_train = drug2_data[train_num]\n",
    "    torch.save(drug2_data_train,'data_perparation/folds/Drug2/fold_'+str(i)+'2_'+'x_train.pt')\n",
    "    torch.save(drug2_data_test,'data_perparation/folds/Drug2/fold_'+str(i)+'2_'+'x_test.pt')\n",
    "    drug2_loader_train = DataLoader(drug2_data_train, batch_size=TRAIN_BATCH_SIZE, shuffle=None)\n",
    "    drug2_loader_test = DataLoader(drug2_data_test, batch_size=TRAIN_BATCH_SIZE, shuffle=None)\n",
    "\n",
    "    \n",
    "    #model = AttentiveFP(in_channels=9, hidden_channels=256, out_channels=2,\n",
    "                    #num_layers=3, num_timesteps=1,\n",
    "                    #dropout= 0.05115077176824934).to(device)\n",
    "    modeling = TransfGNN\n",
    "   \n",
    "    model = modeling().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "    model_file_name = 'best_models/TransfGNN_model' + str(i) + '.model'\n",
    "    # result_file_name = 'data/result/GCNNet(DrugA_DrugB)' + str(i) + '--result_' + datafile +  '.csv'\n",
    "    # file_AUCs = 'data/result/GCNNet(DrugA_DrugB)' + str(i) + '--AUCs--' + datafile + '.txt'\n",
    "    # AUCs = ('Epoch\\tAUC_dev\\tPR_AUC\\tACC\\tBACC\\tPREC\\tTPR\\tKAPPA\\tRECALL')\n",
    "    # with open(file_AUCs, 'w') as f:\n",
    "    #     f.write(AUCs + '\\n')\n",
    "\n",
    "    best_auc = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train(model, device, drug1_loader_train, drug2_loader_train, optimizer, epoch + 1)\n",
    "        T, S, Y = predicting(model, device, drug1_loader_test, drug2_loader_test)\n",
    "        # T is correct label\n",
    "        # S is predict score\n",
    "        # Y is predict label\n",
    "\n",
    "        # compute preformence\n",
    "        AUC = roc_auc_score(T, S)\n",
    "        precision, recall, threshold = metrics.precision_recall_curve(T, S)\n",
    "        PR_AUC = metrics.auc(recall, precision)\n",
    "        BACC = balanced_accuracy_score(T, Y)\n",
    "        tn, fp, fn, tp = confusion_matrix(T, Y).ravel()\n",
    "        TPR = tp / (tp + fn)\n",
    "        PREC = precision_score(T, Y)\n",
    "        ACC = accuracy_score(T, Y)\n",
    "        KAPPA = cohen_kappa_score(T, Y)\n",
    "        recall = recall_score(T, Y)\n",
    "\n",
    "        # save data\n",
    "        AUCs = [epoch, AUC, PR_AUC, ACC, BACC, PREC, TPR, KAPPA, recall]\n",
    "        # save_AUCs(AUCs, file_AUCs)\n",
    "        ret = [rmse(T, S), mse(T, S), pearson(T, S), spearman(T, S), ci(T, S)]\n",
    "        if best_auc < ACC:\n",
    "            best_auc = ACC\n",
    "            print(best_auc)\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            # independent_num = []\n",
    "            # independent_num.append(test_num)\n",
    "            # independent_num.append(T)\n",
    "            # independent_num.append(Y)\n",
    "            # independent_num.append(S)\n",
    "            # txtDF = pd.DataFrame(data=independent_num)\n",
    "            # txtDF.to_csv(result_file_name, index=False, header=False)\n",
    "\n",
    "    results.append(best_auc)\n",
    "res_arr = np.asarray(results)\n",
    "print(np.average(res_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code uses GPU...\n",
      "Pre-processed data found: data_perparation/processed/new_labels_0_10_drug1.pt, loading ...\n",
      "Pre-processed data found: data_perparation/processed/new_labels_0_10_drug2.pt, loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed/anaconda3/envs/mlenv/lib/python3.7/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make prediction for 2483 samples...\n",
      "0.9876854547413205 0.9544905356423681 0.95454890579918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed/anaconda3/envs/mlenv/lib/python3.7/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make prediction for 2483 samples...\n",
      "0.9206047129227942 0.8437374144180427 0.8441434716911511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed/anaconda3/envs/mlenv/lib/python3.7/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make prediction for 2483 samples...\n",
      "0.9224997724997724 0.8417237213048732 0.8421769171769171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed/anaconda3/envs/mlenv/lib/python3.7/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make prediction for 2483 samples...\n",
      "0.9190383931607072 0.8356826419653645 0.835698138867333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed/anaconda3/envs/mlenv/lib/python3.7/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make prediction for 2483 samples...\n",
      "0.9183967345455492 0.8445428916633105 0.8440641669586886\n",
      "0.7279710952975753\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "#from models.gat import GATNet\n",
    "from models.gat_gcn_test import GAT_GCN\n",
    "from models.attentionFP import AttentiveFP\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "from utils_test import *\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, roc_auc_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "def predicting(model, device, drug1_loader_test, drug2_loader_test):\n",
    "    #model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    total_prelabels = torch.Tensor()\n",
    "    print('Make prediction for {} samples...'.format(len(drug1_loader_test.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for data in zip(drug1_loader_test, drug2_loader_test):\n",
    "            data1 = data[0]\n",
    "            data2 = data[1]\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            output = model(data1, data2) #data.x.float(), data.edge_index,data.batch\n",
    "            ys = F.softmax(output, 1).to('cpu').data.numpy()\n",
    "            predicted_labels = list(map(lambda x: np.argmax(x), ys))\n",
    "            predicted_scores = list(map(lambda x: x[1], ys))\n",
    "            total_preds = torch.cat((total_preds, torch.Tensor(predicted_scores)), 0)\n",
    "            total_prelabels = torch.cat((total_prelabels, torch.Tensor(predicted_labels)), 0)\n",
    "            total_labels = torch.cat((total_labels, data1.y.view(-1, 1).cpu()), 0)\n",
    "    return total_labels.numpy().flatten(), total_preds.numpy().flatten(), total_prelabels.numpy().flatten()\n",
    "\n",
    "\n",
    "# CPU or GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('The code uses GPU...')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('The code uses CPU!!!')\n",
    "\n",
    "results = []\n",
    "TEST_BATCH_SIZE = 256\n",
    "drug1_data = TestbedDataset(root='data_perparation', dataset=datafile + '_drug1')\n",
    "drug2_data = TestbedDataset(root='data_perparation', dataset=datafile + '_drug2')\n",
    "\n",
    "for i in range(5):\n",
    "    model = TransfGNN().to(device)\n",
    "    path = 'best_models/TransfGNN_model' + str(i) + '.model'\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    drug1_data_test = torch.load('data_perparation/folds/Drug1/fold_'+str(i)+'1_'+'x_test.pt')\n",
    "    drug1_loader_test = DataLoader(drug1_data_test, batch_size=TEST_BATCH_SIZE, shuffle=None)\n",
    "    drug2_data_test = torch.load('data_perparation/folds/Drug2/fold_'+str(i)+'2_'+'x_test.pt')\n",
    "    drug2_loader_test = DataLoader(drug2_data_test, batch_size=TEST_BATCH_SIZE, shuffle=None)\n",
    "    T, S, Y = predicting(model, device, drug1_loader_test, drug2_loader_test)\n",
    "    AUC = roc_auc_score(T, S)\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(T, S)\n",
    "    PR_AUC = metrics.auc(recall, precision)\n",
    "    BACC = balanced_accuracy_score(T, Y)\n",
    "    tn, fp, fn, tp = confusion_matrix(T, Y).ravel()\n",
    "    TPR = tp / (tp + fn)\n",
    "    PREC = precision_score(T, Y)\n",
    "    ACC = accuracy_score(T, Y)\n",
    "    print(AUC,ACC,BACC)\n",
    "    KAPPA = cohen_kappa_score(T, Y)\n",
    "    recall = recall_score(T, Y)\n",
    "\n",
    "    # save data\n",
    "    AUCs = [AUC, PR_AUC, ACC, BACC, PREC, TPR, KAPPA, recall]\n",
    "    results.append(KAPPA)\n",
    "res_arr = np.asarray(results)\n",
    "print(np.average(res_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
